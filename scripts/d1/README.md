# D1 Migration Guide

This folder contains a starter migration path from Supabase/Postgres to Cloudflare D1 for this project.

## Files

- `001_schema.sql`: D1-compatible schema for app tables and views.
- `upload-r2.ps1`: bulk upload script for restoring files to Cloudflare R2.

## 1) Create D1 + R2

Run once:

```powershell
npx wrangler d1 create book365-db
npx wrangler r2 bucket create book365-files
```

Then add bindings to `wrangler.toml`:

```toml
[[d1_databases]]
binding = "DB"
database_name = "book365-db"
database_id = "<YOUR_D1_DATABASE_ID>"

[[r2_buckets]]
binding = "BOOK_FILES"
bucket_name = "book365-files"
```

## 2) Apply schema

```powershell
npx wrangler d1 execute book365-db --remote --file scripts/d1/001_schema.sql
```

## 3) Import data from Supabase backup

Use your downloaded DB backup, but convert data types/SQL to SQLite form before running inserts.

Important conversion notes:

- `UUID`/`TIMESTAMPTZ`/`JSONB`/`TEXT[]` become `TEXT`.
- Remove RLS/policy/function/plpgsql sections.
- `gen_random_uuid()` values should be replaced with explicit IDs (or generated by import script).
- `COPY ... FROM STDIN` must be converted to `INSERT INTO ... VALUES ...`.

Recommended practical method:

1. Convert each table into CSV.
2. Load table-by-table with an import tool or generated INSERT statements.
3. Verify row counts for all major tables:
   - `"Booklist"`
   - `study_notes`
   - `reading_list_full`
   - `book_tracking`
   - `book_highlights`
   - `book_notes`
   - `ai_book_analysis`
   - `user_list`
   - `admin_users`
   - `admin_settings`

## 4) Restore storage to R2

If your downloaded storage export is local, run:

```powershell
.\scripts\d1\upload-r2.ps1 -SourceDir "C:\path\to\storage-export" -Bucket "book365-files"
```

The script preserves folder structure as object keys.

## 5) App code differences you still need to handle

Current app code uses Supabase-specific features that D1 does not provide directly:

- Supabase Auth session APIs
- Supabase Storage signed URLs
- Supabase RPC calls:
  - `record_book_click`
  - `record_login`
  - `update_reading_progress`
  - `touch_ai_analysis_access`
  - `cleanup_old_ai_analysis`

You will need to replace those with:

- Cloudflare Auth strategy (or keep Supabase Auth separately)
- R2 access patterns
- Direct SQL against D1 from Worker routes

## 6) SQL equivalents for previous RPC calls (for app rewrite)

`record_book_click`:

```sql
INSERT INTO book_clicks (book_id, user_id, click_type, ip_address, user_agent)
VALUES (?, ?, ?, ?, ?);
```

`record_login`:

```sql
INSERT INTO login_tracking (user_id, ip_address, user_agent, session_id)
VALUES (?, ?, ?, ?);
```

`touch_ai_analysis_access`:

```sql
UPDATE ai_book_analysis
SET last_accessed_at = CURRENT_TIMESTAMP
WHERE id = ? AND user_id = ?;
```

`cleanup_old_ai_analysis`:

```sql
DELETE FROM ai_book_analysis
WHERE last_accessed_at < datetime('now', '-' || ? || ' days');
```

`update_reading_progress`:

```sql
INSERT INTO book_tracking (
  user_id, book_id, current_page, total_pages, progress_percentage, last_read_at, updated_at
)
VALUES (
  ?, ?, ?, ?, CASE WHEN ? > 0 THEN (CAST(? AS REAL) / CAST(? AS REAL)) * 100 ELSE 0 END, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP
)
ON CONFLICT(user_id, book_id) DO UPDATE SET
  current_page = MAX(book_tracking.current_page, excluded.current_page),
  total_pages = excluded.total_pages,
  progress_percentage = CASE
    WHEN excluded.total_pages > 0 THEN (CAST(MAX(book_tracking.current_page, excluded.current_page) AS REAL) / CAST(excluded.total_pages AS REAL)) * 100
    ELSE 0
  END,
  last_read_at = CURRENT_TIMESTAMP,
  updated_at = CURRENT_TIMESTAMP;
```

